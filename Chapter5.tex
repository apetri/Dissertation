%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Cosmological parameter inference}
%\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\rightmark}}
 \thispagestyle{plain}
\setlength{\parindent}{10mm}
\label{chp:5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this Chapter we introduce the Standard Model parameter inference techniques used in this work. The parameter inference procedure from WL observations starts with the construction of $\kappa$ images from galaxy shear catalogs. Image features $\bb{d}$ are then extracted from the reconstructed images. When a forward model $\bb{d}(\bb{p})$ that relates features to $\Lambda$CDM parameters $\bb{p}$ is specified, an estimate of the parameters $\bbh{p}$ can be derived from the measured feature $\bbh{d}$ in a Bayesian fashion. In this Chapter we review the Bayesian probabilistic framework and we study parameter constraints from WL using the image features discussed in Chapter \ref{chp:4}. We also discuss some of the numerical and physical effects that lead to the degradation of confidence intervals, suggesting possible mitigation techniques.    

\section{Bayesian formalism}
\label{sec:5:bayes}
In this section we describe the Bayesian probabilistic framework on which we base the $\Lambda$CDM parameter inference. We indicate a $N_d$--dimensional image feature as $\bb{d}$ and a $N_\pi$--dimensional tuple of $\Lambda$CDM parameters (see Table \ref{tab:1:cosmopar}) as $\bb{p}$. We also denote feature estimates from a simulated $\kappa$ field of view as $\bbh{d}$, feature measurements from an actual observation as $\dobs$ and the resulting parameter estimates as $\bbh{p}$. We assume the existence of a forward model $\bb{d}(\bb{p})$, which can be calculated using our WL simulation pipeline described in \S~\ref{sec:3:lt} or, in special cases such as for the $\kappa$ power spectrum, using analytical codes like NICAEA \citep{Nicaea,Nicaea17}. According to Bayes theorem, the likelihood $\lik{\bbh{p}}{\dobs}$ of a parameter estimate $\bbh{p}$ given an observation $\dobs$ is given by

\begin{equation}
\label{eq:5:bayesthm}
\lik{\bbh{p}}{\dobs,\bb{d}(\bb{p})} = \frac{\lik{\dobs}{\bbh{p},\bb{d}(\bb{p})}\Pi(\bbh{p})}{\mathcal{L}(\dobs)}
\end{equation}
%
In equation (\ref{eq:5:bayesthm}), $\Pi$ encodes prior information on the parameters coming from independent probes independent from WL (such as CMB experiments) and $\mathcal{L}(\dobs)$ is the overall likelihood of the observation, which plays the simple role of a $\bb{p}$--independent normalization factor in the parameter likelihood (\ref{eq:5:bayesthm}). In the prosecution of this work this normalization factor will be ignored. We assume a Gaussian feature likelihood 

\begin{equation}
\label{eq:5:gaussfeatlik}
\lik{\dobs}{\bbh{p},\bb{d}(\bb{p})} = \frac{1}{(2\pi)^{N_d/2}\vert\bb{C}\vert^{1/2}}\exp\left(-\frac{1}{2}(\dobs-\bb{d}(\bb{p}))^T\bb{C}^{-1}(\dobs-\bb{d}(\bb{p}))\right), 
\end{equation}
%
where $\bb{C}$ is the $\bb{p}$--independent feature--feature covariance matrix. The Gaussian assumption for the data likelihood is justified by the Central Limit Theorem because measured image features are averaged over a large number of $\theta_{\rm FOV}=3.5\,{\rm deg}$ fields of view (13 for CFHTLenS and over 1000 for LSST). We do not discuss covariance matrix dependence on $\bb{p}$ in this work, reserving the topic for future investigation. 

Parameter confidence intervals can be obtained looking at surfaces in $\bb{p}$ space with constant $\mathcal{L}(\bbh{p})$. We define an $N\sigma$ confidence interval to be the region in $\bb{p}$ space in which $\mathcal{L}>\mathcal{L}_N$. The likelihood confidence levels are defined as 

\begin{equation}
\label{eq:5:liklevel}
\int_{\mathcal{L}>\mathcal{L}_N}\lik{\bbh{p}}{\dobs,\bb{d}(\bb{p})} d\bbh{p} = \frac{1}{\sqrt{2\pi}}\int_{-N}^Ne^{-x^2/2}dx
\end{equation}  
%
Note that this definition of $N\sigma$ confidence intervals (see Figure \ref{fig:5:contsample} for a visual example) corresponds to the commonly accepted one when $\mathcal{L}(\bbh{p})$ is a multivariate Gaussian in $\bbh{p}$. In this case, calling $\bbh{p}_0$ the location of the likelihood peak, the matrix $\bb{\Sigma}$ defined by

\begin{equation}
\label{eq:5:parcov}
\left(\Sigma^{-1}\right)_{\alpha\beta} = -\left(\frac{\partial^2\log\mathcal{L}(\bbh{p})}{\partial\h{p}_\alpha\partial\h{p}_\beta}\right)_{\bbh{p}=\bbh{p}_0},
\end{equation}
%
is the covariance of the parameter estimate. If the parameter likelihood is not a multivariate Gaussian, we can still use the peak location $\bbh{p}_0$ and the matrix (\ref{eq:5:parcov}) as estimates of the parameters and of their covariance matrix, although a complete characterization of the parameter space through the confidence intervals defined in (\ref{eq:5:liklevel}) is preferred. Confidence intervals can be calculated by drawing samples from $\mathcal{L}(\bbh{p})$ using Markov Chain Monte Carlo (MCMC) techniques, which are implemented by many user--friendly software packages, such as \ttt{emcee} \citep{emcee}. An example of 1,2 and 3$\sigma$ confidence contours on the parameter doublet $(\Omega_m,\sigma_8)$ is shown in Figure \ref{fig:5:contsample}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{Figures/png/cfht_contoursOmega_m-sigma8_sample.png}
\end{center}
\caption{Sample 1 (blue), 2 (green) and 3$\sigma$ (red) example confidence contours on $(\Omega_m,\sigma_8)$. The gray scale refers to the value of the parameter likelihood $\mathcal{L}(\bbh{p})$.}
\label{fig:5:contsample}
\end{figure}

\subsection{Fisher matrix approximation}
\label{sec:5:fisher}
Parameter inference becomes simpler if the forward model $\bb{d}(\bb{p})$ is linear. Linearity can be safely assumed if confidence intervals are localized around the peak of the likelihood, which is the case for large scale surveys. Under the linearity assumption, we can write 

\begin{equation}
\label{eq:5:linapprox}
\bb{d}(\bb{p}) = \bb{d}_0 + \bb{M}(\bb{p}-\bb{p}_0) + O(\vert\bb{p}-\bb{p}_0\vert^2).
\end{equation}          
%
Assuming a flat prior $\Pi(\bbh{p})$ and substituting (\ref{eq:5:linapprox}) into (\ref{eq:5:gaussfeatlik}) we get, for the $\bb{p}$--dependent part of the likelihood

\begin{equation}
\label{eq:5:linapprox-lik}
-2\log\mathcal{L}(\bb{p}) = \left[\dobs-\bb{d}_0-\bb{M}(\bb{p}-\bb{p}_0)\right]^T\bb{\Psi}\left[\dobs-\bb{d}_0-\bb{M}(\bb{p}-\bb{p}_0)\right]
\end{equation}
%
We used the notation $\bb{\Psi}=\bb{C}^{-1}$. We can estimate the peak location of the likelihood $\bbh{p}_0$ and its covariance $\bb{\Sigma}$ from (\ref{eq:5:linapprox-lik}) using (\ref{eq:5:parcov}):

\begin{equation}
\label{eq:5:linapprox-peak}
\bbh{p}_0 = \bb{p}_0 + \left(\bb{M}^T\bb{\Psi}\bb{M}\right)^{-1}\bb{M}^T\bb{\Psi}\left(\dobs-\bb{d}_0\right)
\end{equation}
%
\begin{equation}
\label{eq:5:linapprox-cov}
\bb{\Sigma} = \bb{F}^{-1} \equiv \left(\bb{M}^T\bb{\Psi}\bb{M}\right)^{-1} 
\end{equation}
%
Equations (\ref{eq:5:linapprox-peak}), (\ref{eq:5:linapprox-cov}) take the name of \textit{Fisher matrix} approximation and $\bb{F}\equiv\bb{M}^T\bb{\Psi}\bb{M}$ is usually referred to as \textit{Fisher information matrix}. When prior information on the parameters is available, the estimates for the likelihood peak and covariance matrix have to be modified. If the prior is a multivariate Gaussian with distribution

\begin{equation}
\label{eq:5:gaussprior}
\Pi(\bb{p}) = \sqrt{\frac{\vert\bb{F}_\Pi\vert}{(2\pi)^{N_\pi}}}\exp\left(-\frac{1}{2}(\bb{p}-\bb{p}_\Pi)^T\bb{F}_\Pi(\bb{p}-\bb{p}_\Pi)\right),
\end{equation}
%
we can write

\begin{equation}
\label{eq:5:linapprox-peak-prior}
\bbh{p}_0 = \left(\bb{F}+\bb{F}_\Pi\right)^{-1}\left[\bb{F}_\Pi\bb{p}_\Pi + \bb{F}\bb{p}_0 + \bb{M}^T\bb{\Psi}(\bbh{d}-\bb{d}_0) \right]
\end{equation}
%
\begin{equation}
\label{eq:5:linapprox-cov-prior}
\bb{\Sigma} = \left(\bb{F}+\bb{F}_\Pi\right)^{-1} 
\end{equation}
%
Equation (\ref{eq:5:linapprox-cov-prior}) states that, if the parameter prior is independent from the WL observation, inverse parameter covariances have to be added in quadrature. If the parameter likelihood and prior peak at the same location $\bb{p}_0=\bb{p}_\Pi$, equation (\ref{eq:5:linapprox-peak-prior}) reduces to (\ref{eq:5:linapprox-peak}) with a modified Fisher information matrix $\bb{F}+\bb{F}_\Pi$.    

\section{Error degradation induced by noise in the covariance matrix}
\label{sec:5:degrade}
In the previous derivation of parameter estimates (\ref{eq:5:linapprox-peak}) and covariances (\ref{eq:5:linapprox-cov}), we have assumed perfect knowledge of the feature--feature covariance matrix $\bb{C}$ and of its inverse $\bb{\Psi}$. Although smooth models exist for the covariance matrix of the $\kappa$ power spectrum (see (\ref{eq:4:powercov-gauss}) for example), the same is not true for the higher order features described in Chapter \ref{chp:4}. When such smooth models are not readily available one can obtain an estimate $\bbh{C}$ of $\bb{C}$ from simulations. The estimate can then be used to calculate an approximate feature likelihood (\ref{eq:5:gaussfeatlik}). If one choses this way to proceed, the noise in the estimator $\bbh{C},\bbh{\Psi}$ carries over to the parameter estimate $\bbh{p}_0$ and covariance $\bb{\Sigma}$, which is then only available as a noisy estimate $\bbh{\Sigma}$. If simulations and observations are independent from each other, the parameter estimate $\bbh{p}_0$ is unbiased (within the limits of the linear approximation (\ref{eq:5:linapprox})). The parameter covariance estimator defined by

\begin{equation}
\label{eq:5:pcov-est-1}
\bbh{\Sigma}_1 = \bbh{F}^{-1},
\end{equation}
%
on the other hand, is a biased estimate of $\bb{\Sigma}$, as we will see later in the Chapter. The unbiased version of (\ref{eq:5:pcov-est-1}) is the correct estimation of the error--bar to assign to $\bbh{p}_0$ only if the scatter of the estimator (\ref{eq:5:linapprox-peak}) is equal to $\Sigma$. We will see, unfortunately, that this is not true. With the simplifying assumption that $\langle\dobs-\bb{d}_0\rangle=0$, the scatter of (\ref{eq:5:linapprox-peak}) is given by

\begin{equation}
\label{eq:5:peak-scatter}
\left\langle\delta\bbh{p}_0\delta\bbh{p}_0^T\right\rangle = \left\langle\bbh{F}^{-1}\bb{M}^T\bbh{\Psi}(\dobs-\bb{d}_0)(\dobs-\bb{d}_0)^T\bbh{\Psi}\bb{M}\bbh{F}^{-1}\right\rangle.
\end{equation}
%
In equation (\ref{eq:5:peak-scatter}), the expectation value has to be taken with respect of both the observations and the simulations, which are both affected by noise but are uncorrelated. To have an idea of the magnitude of (\ref{eq:5:peak-scatter}), we can take the expectation value over the observation and focus ourselves on the the noise introduced exclusively by the simulations. We will use the fact

\begin{equation}
\label{eq:5:expobs}
\left\langle(\dobs-\bb{d}_0)(\dobs-\bb{d}_0)^T\right\rangle = \bb{C}
\end{equation}
%
to produce a noisy estimator of the $\bbh{p}_0$ scatter, which we call $\bbh{\Sigma}_2$. The latter quantity is defined as 

\begin{equation}
\label{eq:5:pcov-est-2}
\bbh{\Sigma}_2 = \bbh{F}^{-1}\bb{M}^T\bbh{\Psi}\bb{C}\bbh{\Psi}\bb{M}\bbh{F}^{-1} 
\end{equation} 
%
In the next sub--section we are going to show how the expectation values of (\ref{eq:5:pcov-est-1}) and (\ref{eq:5:pcov-est-2}) over the simulations can be calculated explicitly under a Gaussianity assumption.    

\subsection{Covariance matrix estimation}
In order to produce estimates of the feature--feature covariance matrix $\bb{C}$, we use our WL simulation pipeline (described in Chapter \ref{sec:3:lt}), whose products are pseudo--independent realizations of $\kappa$ in a WL field of view. We measure the feature $\bbh{d}_r$ from each simulated image using the techniques described in Chapter \ref{chp:4} and we produce an estimator for the covariance matrix $\bbh{C}$ based on simulated ensembles of $N_r$ image realizations:

\begin{equation}
\label{eq:5:meanest-sim}
\bbh{d}_{\rm mean} = \frac{1}{N_r}\sum_{r=1}^{N_r}\bbh{d}_r 
\end{equation}
%
\begin{equation}
\label{eq:5:covestest-sim}
\bbh{C} = \frac{1}{n}\sum_{r=1}^{N_r}\left(\bbh{d}_r-\bbh{d}_{\rm mean}\right)^T\left(\bbh{d}_r-\bbh{d}_{\rm mean}\right) 
\end{equation}
%
We indicated the effective number of degrees of freedom in the ensemble as $n=N_r-1$. This effective number is smaller than $N_r$ because the mean feature $\bbh{d}_{\rm mean}$ is not known and has to be estimated from the ensembles themselves. If the feature estimate $\bbh{d}_r$ is drawn from a multivariate Gaussian distribution with covariance matrix $\bb{C}$, the covariance estimate $\bbh{C}$ is distributed according to the Wishart probability density \citep{Taylor12,Taylor14,MasumotoWishart}. A functional form for the Wishart density function, $\lik{\bbh{C}}{\bb{C},n}$, can be obtained from its characteristic function 

\begin{equation}
\label{eq:5:chardef}
\phi(\bb{J}) = \left\langle e^{i\Tr(\bb{J}\bbh{C})}\right\rangle
\end{equation}
%
We can derive an expression of $\lik{\bbh{C}}{\bb{C},n}$ from $\phi(\bb{J})$ performing an inverse Fourier transform in matrix space (much like the inversion described in (\ref{eq:4:characteristic-inverse})). The characteristic function $\phi$ can be evaluated from the moments of the Wishart distribution, which are easily expressed in terms of $\bb{C}$ and $n$ via a straightforward though tedious procedure based on Wick's theorem. After the smoke clears we get (see \citep{Taylor12} for the details)

\begin{equation}
\label{eq:5:charw}
\phi(\bb{J}) = \left\vert\mathds{1}_{N_d\times N_d}-\frac{2i\bb{JC}}{n}\right\vert^{-n/2}
\end{equation}
%
The inverse Fourier transform leads to the functional form of the Wishart density function $\lik{\bbh{C}}{\bb{C},n} = \mathcal{W}(\bb{C},\bbh{C},n)$, with

\begin{equation}
\label{eq:5:wishart}
\mathcal{W}(\bb{C},\bbh{C},n) =  \left(\frac{n^{nN_d/2}\vert \bb{C}\vert^{-n/2}\vert\bbh{C}\vert^{(n-N_d-1)/2}}{2^{nN_d/2}\Gamma_{N_d}(n/2)}\right)\exp\left(-\frac{n}{2}\Tr(\bbh{C}\bb{C}^{-1})\right)
\end{equation}
%
The generalized gamma function $\Gamma_N$ is defined via an integral over the positive semidefinite $N\times N$ symmetric matrices as 

\begin{equation}
\label{eq:5:gengamma}
\Gamma_N(x) = \int_{\bb{X}>0}d\bb{X}\vert\bb{X}\vert^{x-(N+1)/2}e^{-\Tr\bb{X}}
\end{equation}
%
We can use the functional form of the Wishart density (\ref{eq:5:wishart}) to derive a closed formula for the distribution of the estimate of the inverse $\bbh{\Psi}=\bbh{C}^{-1}$. This can be done by means of a change of variables in matrix space. Following \citep{Taylor12}, we obtain the result $\lik{\bbh{\Psi}}{\bb{\Psi},n} = \mathcal{W}^{-1}(\bbh{\Psi},\bb{\Psi},n)$, with 

\begin{equation}
\label{eq:5:wishart-inv}
\mathcal{W}^{-1}(\bbh{\Psi},\bb{\Psi},n) = \left(\frac{n^{nN_d/2}\vert \bb{\Psi}\vert^{n/2}\vert\bbh{\Psi}\vert^{-(n+N_d+1)/2}}{2^{nN_d/2}\Gamma_{N_d}(n/2)}\right)\exp\left(-\frac{n}{2}\Tr(\bb{\Psi}\bbh{\Psi}^{-1})\right)
\end{equation}
%
Using the expression (\ref{eq:5:wishart-inv}), it can be shown (see \citep{MasumotoWishart}) that the estimate of the inverse covariance $\bbh{\Psi}$ is biased according to

\begin{equation}
\label{eq:5:inv-bias}
\left\langle\bbh{\Psi}\right\rangle = \frac{n\bb{\Psi}}{n-N_d-1}
\end{equation} 
%
Because the moments of the probability distribution of any square sub--matrix of $\bbh{\Psi}$ can be expressed in terms of the relevant elements of $\bb{\Psi}$, $n$ and the combination $\gamma=(n-N_d-1)/2$ (see again \citep{MasumotoWishart}), it can be shown that the rescaled Fisher information estimate $\bbh{F}'$, defined by 

\begin{equation}
\label{eq:5:rescaledF}
\bbh{F}' = \frac{(n+N_\pi-N_d)\bbh{F}}{n},
\end{equation}
%
is distributed as $\lik{\bbh{F}'}{\bb{F}',n,N_\pi,N_d} = \mathcal{W}^{-1}(\bbh{F}',\bb{F}',n+N_\pi-N_d)$. This fact leads immediately to the conclusion that parameter error bar estimates based on (\ref{eq:5:pcov-est-1}) are biased according to 

\begin{equation}
\label{eq:5:pcov-est-1-bias}
\left\langle\bbh{\Sigma}_1\right\rangle = \left(1+\frac{N_\pi-N_d}{n}\right)\bb{\Sigma}
\end{equation}
%
This bias can easily be easily mitigated by applying a suitable correction factor (suggested in equation (\ref{eq:5:pcov-est-1-bias})) to the parameter covariance estimator (\ref{eq:5:pcov-est-1}). This procedure can be used to estimate parameter error bars in an unbiased fashion by relying only on the simulations, can hence be used to obtain approximate forecasts for parameter contours. When analyzing a real observation, however, we are left with a parameter estimate $\bbh{p}_0$ which obtained from the peak of the likelihood. As we are going to see, $\bbh{p}_0$ is drawn from a distribution whose width is larger than $\bb{\Sigma}$. This increased scatter originates from the fact that the feature--feature covariance estimate is noisy: even if the $\bbh{C}$ estimator is unbiased, the estimate of the peak scatter $\bbh{\Sigma}_2$, defined in (\ref{eq:5:pcov-est-2}), is not. Unlike the case for $\bbh{\Sigma}_1$, the expectation value of $\bbh{\Sigma}_2$ cannot be calculated exactly and needs to be approximated. We tackle this issue in the next sub--section.  

\subsection{Perturbative calculation of the estimate scatter}
It is not possible to calculate the expectation value of (\ref{eq:5:pcov-est-2}) analytically because the expression contains both $\bbh{\Psi}$ and $\bbh{F}^{-1}$. In order to evaluate the behavior of the scatter $\bbh{\Sigma}_2$ with $N_d,N_r$, we adopt a perturbative approach in the quantity $\delta\bbh{\Psi}$, defined by

\begin{equation}
\label{eq:5:deltapsi}
\bbh{\Psi} = \bb{\Psi} + \delta\bbh{\Psi}
\end{equation}
%
With this definition, the expression (\ref{eq:5:pcov-est-2}) can be expanded in a power series in $\delta\bbh{\Psi}$. If the moments of the inverse Wishart distribution are known, we can use them to calculate $\langle\bbh{\Sigma}_2\rangle$ at arbitrary orders in $\delta\bbh{\Psi}$. Using the notation

\begin{equation}
\label{eq:5:deltaF}
\delta\bbh{F} = \bb{M}^T\delta\bbh{\Psi}\bb{M},
\end{equation}
%
we can write

\begin{equation}
\label{eq:5:pcov-est-2-pert}
\bbh{\Sigma}_2 = (\bb{F}+\delta\bbh{F})^{-1}\bb{M}^T(\bb{\Psi}+\delta\bbh{\Psi})\bb{C}(\bb{\Psi}+\delta\bbh{\Psi})\bb{M}(\bb{F}+\delta\bbh{F})^{-1}
\end{equation}
%
The series expansion for $(\bb{F}+\delta\bbh{F})^{-1}$, which is given by

\begin{equation}
\label{eq:5:seriesdF}
(\bb{F}+\delta\bbh{F})^{-1} = \sum_{k=0}^\infty (-1)^k(\bb{F}^{-1}\delta\bbh{F})^k\bb{F}^{-1},
\end{equation}
%
provides a straightforward, although algebraically tedious, way to express (\ref{eq:5:pcov-est-2-pert}) at the desired order in $\delta\bbh{\Psi}$. \citep{MasumotoWishart} showed that a series expansion in $\delta\bbh{\Psi}$ is roughly equivalent to a perturbation series in $1/N_r$, with higher connected moments of the inverse Wishart distribution corresponding to higher powers in $1/N_r$. We need to know the moments of $\mathcal{W}^{-1}$ up to quartic order \citep{PetriVariance} to express the expectation value of (\ref{eq:5:pcov-est-2-pert}) at $O(1/N_r^2)$. We quote the expressions for the $\mathcal{W}^{-1}$ moments from \citep{MasumotoWishart}:

\begin{equation}
\label{eq:5:inv-wishart-moments-2}
\left\langle\delta\h{\Psi}_I\delta\h{\Psi}_J\right\rangle = \frac{\Psi_I\Psi_J + \gamma\Psi_{\{ I}\Psi_{J\}}}{(\gamma-1)(2\gamma+1)}
\end{equation}
%
\begin{equation}
\label{eq:5:inv-wishart-moments-3}
\left\langle\delta\h{\Psi}_I\delta\h{\Psi}_J\delta\h{\Psi}_K\right\rangle = \frac{\gamma^2\Psi_{\{ I}\Psi_J\Psi_{K\}}}{(\gamma-1)(\gamma-2)(\gamma+1)(2\gamma+1)}
\end{equation}
%
\begin{equation}
\label{eq:5:inv-wishart-moments-4}
\left\langle\delta\h{\Psi}_I\delta\h{\Psi}_J\delta\h{\Psi}_K\delta\h{\Psi}_L\right\rangle = \frac{\gamma^3(2\gamma^2-5\gamma+9)\Psi_{\{ I}\Psi_{J\}}\Psi_{\{ K}\Psi_{L\}}}{(\gamma-1)(\gamma-2)(\gamma-3)(2\gamma-1)(\gamma+1)(2\gamma+1)(2\gamma+3)}
\end{equation}
%
The adopted notation is the following: we use a capital letter $I=(i_1,i_2)$ to indicate a pair of indexes $i_1,i_2$, and we use curly braces to indicate a symmetrization in the indexes

\begin{equation}
\label{eq:5:symmind}
\Psi_{\{ I}\Psi_{J\}} = \Psi_{i_1j_1}\Psi_{i_2j_2} + \Psi_{i_1j_2}\Psi_{i_2j_1}
\end{equation}  
%
In equations (\ref{eq:5:inv-wishart-moments-2}), (\ref{eq:5:inv-wishart-moments-3}) and (\ref{eq:5:inv-wishart-moments-4}), we kept only the terms which are of order $O(1/N_r^2)$. Looking at the structure of the expression (\ref{eq:5:pcov-est-2-pert}) and at the expressions for the inverse Wishart moments, we conclude that the expectation value $\langle\bbh{\Sigma}_2\rangle$ must be be the sum of terms in the form $f_a(N_d,N_\pi)\bb{\Sigma}/N_r^a$, where $f_a$ a polynomial of $N_d,N_\pi$. Each of these polynomials contains at least one factor proportional to $N_d-N_\pi$ since, if $N_d=N_\pi$, $\langle\bbh{\Sigma}_2\rangle=\bb{\Sigma}$. After expanding (\ref{eq:5:pcov-est-2-pert}) at quadratic, cubic and quartic order in $\delta\bbh{\Psi}$ and  carrying out the calculations, we can separate the contributions to $\langle\bbh{\Sigma}_2\rangle$ due to $O(\delta\bbh{\Psi}^n)$ terms as

\begin{equation}
\label{eq:5:contr-2}
(\delta\bbh{\Psi})^2 \rightarrow \frac{\gamma(N_d-N_\pi)\bb{\Sigma}}{(\gamma-1)(2\gamma+1)}
\end{equation} 
%
\begin{equation}
\label{eq:5:contr-3}
(\delta\bbh{\Psi})^3 \rightarrow -\frac{4(N_d-N_\pi)(1+N_\pi)\bb{\Sigma}}{N_r^2}
\end{equation} 
%
\begin{equation}
\label{eq:5:contr-4}
(\delta\bbh{\Psi})^4 \rightarrow \frac{3(N_d-N_\pi)(1+N_\pi)\bb{\Sigma}}{N_r^2}
\end{equation} 
%
Combining (\ref{eq:5:contr-2}), (\ref{eq:5:contr-3}) and (\ref{eq:5:contr-4}) we finally get 

\begin{equation}
\label{eq:5:pcov-est-2-exp}
\left\langle\bbh{\Sigma}_2\right\rangle = \left(1+\frac{N_d-N_\pi}{N_r}+\frac{(N_d-N_\pi)(N_d-N_\pi+2)}{N_r^2}\right)\bb{\Sigma} + O\left(\frac{1}{N_r^3}\right)
\end{equation} 
%
The result (\ref{eq:5:pcov-est-2-exp}) has an important consequence: although parameter error bars forecast from simulations via (\ref{eq:5:pcov-est-1-bias}) are unbiased, the scatter of the likelihood peak $\bbh{p}_0$ is larger than $\bb{\Sigma}$ by a factor of $\sim 1+N_d/N_r$. This is always the case when we use a noisy estimate of feature covariance matrix obtained with (\ref{eq:5:covestest-sim}). This means that, for high dimensional image features, estimation noise in the covariance matrix severely degrades parameter estimates, and the error bar forecast from simulations is an under--estimate. \citep{Taylor14} proposed an empirical formula for $\langle\bbh{\Sigma}_2\rangle$ which accurately reproduces numerical estimates of the parameter degradation:

\begin{equation}
\label{eq:5:pcov-ext-2-emp}
\left\langle\bbh{\Sigma}_2\right\rangle_{\rm empirical} = \left(\frac{N_r-2}{N_r+N_\pi-N_d-2}\right)\bb{\Sigma}
\end{equation}
%
Note that (\ref{eq:5:pcov-ext-2-emp}) reduces to (\ref{eq:5:pcov-est-2-exp}) when expanded up to order $O(1/N_r^2)$. 
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{Figures/png/curving_nb.png}
\end{center}
\caption{Expectation value of the $w_0$ peak scatter $\h{\Sigma}_{w_0w_0}$ as a function of the number of pseudo--independent realizations $N_r$ used to measure the feature covariance matrix. We consider a variety of features with different dimensionality, including the $\kappa$ power spectrum and peak counts. We show the numerical results obtained with a bootstrapping procedure (points), the $O(1/N_r),O(1/N_r^2)$ perturbation theory predictions from equation (\ref{eq:5:pcov-est-2-exp}) (dashed and thin solid lines respectively) and the empirical result from equation (\ref{eq:5:pcov-ext-2-emp}) (thick solid lines). The asymptotic parameter covariance $\bb{\Sigma}_\infty$, which coincides with the true covariance $\bb{\Sigma}$, has been estimated with a linear regression of $\langle\bbh{\Sigma}\rangle_2$ versus $1/N_r$ using the large $N_r$ tail.} 
\label{fig:5:curvenb}
\end{figure}
%
Figure \ref{fig:5:curvenb} shows the results of a numerical experiment we performed using ensemble bootstrapping. We measured the constraint degradation of the Dark Energy equation of state parameter $w_0$. The Figure shows that, for ratios $N_d/N_r$ close to unity, the scatter of the $w_0$ estimate is be significantly bigger than the forecast covariance $\bb{\Sigma}$ (see \citep{DodelsonSchneider13,PetriVariance}). This numerical degradation brings up the necessity of dimensionality reduction: high dimensional features likely contain more information about cosmology but, since their covariance matrix has to measured from simulations, constraint degradation results in larger error bars. A good compromise is to find a way to construct low dimensional features which retain as much information on cosmology as possible. We propose some possible recipes for this non trivial task later in the Chapter.  

\section{Pseudo--independence of realizations}
\label{sec:5:pseudo}
The analytical results illustrated in \S~\ref{sec:5:bayes}, \S~\ref{sec:5:degrade} are based on the assumption that the realizations in the $\kappa$ ensembles are independent. Because these ensembles are built with the sampling procedure described in \S~\ref{sec:3:sampling}, which makes use of $N_s$ independent $N$--body simulations to construct a large number $N_r\gg N_s$ of WL fields, the realizations are all independent. Moreover, if $N_s$ is small, it is not guaranteed that cosmic variance is sampled in an unbiased way. Given the size ($c/H_0\sim 3\,{\rm Gpc}$) of the present Hubble horizon, this might be an issue for our simulations, which have a box size of $L_b\approx 250\,{\rm Mpc}/h$. 
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{Figures/png/ps_pdf.png}
\end{center}
\caption{Distribution of the $\kappa$ power spectrum at four distinct values of $\ell$. We plot the histogram of 1000 $P_{\kappa\kappa}$ realizations in ensembles built with different $N_s$ (solid colored lines). We also show the distribution of $\sim 10^5$ $P_{\kappa\kappa}$ realizations in an ensemble built from a single $N$--body simulation ($L_b=240\,{\rm Mpc}/h$) (dashed black line). The simulations on which this Figure is based are taken from the \ttt{CovarianceBatch} set (see Appendix).}
\label{fig:5:pspdf}
\end{figure}
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{Figures/png/means_ns.png} \includegraphics[scale=0.35]{Figures/png/scaling_ns.png}
\end{center}
\caption{Top panel: mean feature measured from $N_r=1000$ realizations drawn from ensembles build with varying $N_s$. We show the $\kappa$ power spectrum measured at three selected $\ell$ values and the peak counts of three different heights $\kappa_0$ (colored lines). The mean feature is plotted in units of the statistical error measured from the ensemble variance. We also draw a dashed black line which shows the tolerance of 10\% of the statistical error. Bottom panel: asymptotic parameter variance on $w_0$ plotted against the number of independent simulations $N_s$ used to construct the WL ensembles. The asymptotic variance has been obtained with a linear regression of the bootstrapped $\langle\bbh{\Sigma}_2\rangle$ versus $1/N_r$ for large $N_r$. The trends are shown in units of the mean over $N_s$ for two different binning choices of the power spectrum (black, red) and for the peak counts (green). The simulations on which this Figure is based are taken from the \ttt{CovarianceBatch} set (see Appendix).}
\label{fig:5:mcns}
\end{figure}
%
The effect of biased sampling is evident in Figure \ref{fig:5:pspdf}, which shows the distribution of the $\kappa$ power spectrum at selected $\ell$ values over different realizations. We can clearly see that, if the WL sampling is based on a single $N$--body simulation ($N_s=1$), the peak of the distribution of $P_{\kappa\kappa}$ varies among ensembles. Different initial conditions  for the $N$--body simulation lead to different estimates of the small scale power spectrum ($\ell\gtrsim 1000$). The same distinction is not evident on larger scales ($\ell\lesssim 400$). Figure \ref{fig:5:pspdf} also shows that, with the chosen box size, $N_s=5$ is sufficient to obtain an unbiased sample of the $\kappa$ power spectrum up to $\ell\approx5000$. Another aspect of biased feature sampling is shown in the top panel of Figure \ref{fig:5:mcns}, which shows the mean of selected features as a function of the number of independent simulations $N_s$. The plot shows that for features that trace density fluctuations in the linear regime, such as power spectra at small $\ell$ and low height peaks, as few as $N_s=2$ simulations (with $L_b=240\,{\rm Mpc}/h$) are sufficient for the bias to be within 10\% of the statistical error. On the contrary, for features that trace the non--linear cosmic density such as power spectra at high $\ell$ and high peaks, the required number of $N$--body simulations (for the same $0.1\sigma$ reference tolerance) is of the order of 10.    
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{Figures/png/scaling_nr.png}
\end{center}
\caption{Degradation in the $w_0$ constraint as a function of $N_r$ for different choice of features. The features measured from ensembles built with $N_s=1$. The asymptotic variance $\bb{\Sigma}_{w_0w_0,\infty}$ was estimated using the value of $\langle\bbh{\Sigma}_2\rangle$ at $N_r=128,000$. We show the trends for small (black) and large (blue) scale power spectra and large threshold peak counts for non--smoothed (green) and $1'$ smoothed (red) $\kappa$ images. The simulations on which this Figure is based are taken from the \ttt{CovarianceBatch} set (see Appendix).}
\label{fig:5:pseudonr}
\end{figure}
%
The lower panel of Figure \ref{fig:5:mcns} shows that the pseudo--independence of $\kappa$ realizations at small $N_s$ does not affect the estimation of the asymptotic parameter variance $\bb{\Sigma}$, which is correctly obtained at $N_s=2$ already (modulo statistical fluctuations). 

If $N_s=1$, the WL realizations cannot be considered all independent, as Figure \ref{fig:5:pseudonr} shows. The Figure highlights the fact that, when the WL sampling is based on a single $N$--body simulation, features measured from the $\kappa$ ensemble start to be correlated for large $N_r$. Regardless of which feature one considers, the error bar degradation $\langle\bbh{\Sigma}_2\rangle-\bb{\Sigma}$ behaves asymptotically as $1/N_r$ in the limit of independent realizations. What our numerical experiment shows, on the other hand, is a flattening of the trend at high $N_r$, hinting to correlations among the realizations in the ensemble. The $1/N_r$ behavior is broken at different $N_r$ depending on which feature we are looking at. Large scale $\kappa$ power spectra drawn from a single box can be considered independent for $N_r<{\rm few}\times 10^3$, while non--linear statistics, such power spectra at high $\ell$ and peak counts, are independent up to $N_r<{\rm few}\times 10^4$. These numbers should be interpreted as orders of magnitude and are referred to the size of the boxes $L_b=240\,{\rm Mpc}/h$ used in this work. Different box sizes will likely modify the scale $N_r$ on which deviations from the $1/N_r$ behavior start to appear \citep{NbodyLB,PetriVariance}.

\section{Dimensionality reduction}
\label{sec:5:dimred}
In the previous sections we discussed how noise in the feature covariance matrix degrades the inference of cosmological parameters. A way to mitigate this effect (which is expressed by equations (\ref{eq:5:pcov-est-2-exp}) and (\ref{eq:5:pcov-ext-2-emp})) is to use a large number $N_r$ of independent WL realizations. In section \S~\ref{sec:5:pseudo}, however, we saw that there is a limit on the number of independent realizations one can generate from a single $N$--body simulation. Because the degradation in the error bars, at first order, is proportional to the feature dimensionality $N_d$, techniques that capture the same cosmological information with a smaller number of dimensions $N_c<N_d$ assume particular relevance. In this section we will explore one of such techniques, which takes the name of \textit{Principal Component Analysis} (PCA) \citep{astroMLText}. PCA projects image features onto a lower dimensional space which hopefully retains most of the original information. We indicate with $\bb{D}$ the $N_M\times N_d$ feature matrix, in which each of the $N_M$ rows represents an image feature in a different $\Lambda$CDM model. We perform a SVD decomposition \citep{scipy,astroMLText} of $\bb{D}$ by writing it in the form
%  
\begin{equation}
\label{eq:5:svd}
\bb{D} = \bb{L\Lambda R}
\end{equation}
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{Figures/png/w_power_spectrum_pca.png} \includegraphics[scale=0.35]{Figures/png/w_peaks_pca.png}
\includegraphics[scale=0.35]{Figures/png/w_moments_pca.png}
\end{center}
\caption{$1\sigma$ constraint on $w_0$ as function of the number of PCA components $N_c$ used in the projection. We show the case for the $\kappa$ power spectrum (top left) binned with 15 $\ell$ bins per redshift, the $\kappa$ peak counts (top right) binned in 45 $\kappa$ height bins per redshift, and the 9 moments (bottom) defined in (\ref{eq:4:poly-quad}), (\ref{eq:4:poly-cubic}) and (\ref{eq:4:poly-quartic}). We consider the single redshift case (thin lines) and the tomographic case (thick line). The constraints are plotted in units of the non--projected constraint, obtained from the full feature space. The feature covariance matrix has been measured from an ensemble made of $N_r=16,000$ realizations in order to ignore constraint degradation effects. We use $N_M=100$ different choices of $(\Omega_m,w_0,\sigma_8)$ to perform the PCA \citep{PetriPhotoZ}, and we choose the whitening parameters as $\mu_d=\sigma_d=\sum_p D_{pd}/N_M$.}
\label{fig:5:pca}
\end{figure}
%
where $\bb{L}$ is $N_M\times K$, $\bb{R}$ is $K\times N_d$ and $\bb{\Lambda}$ is a diagonal $K\times K$ matrix. We adopted the notation $K={\rm min}(N_d,N_M)$. The diagonal components of $\bb{\Lambda}=(\Lambda_1,...,\Lambda_K)$, assumed sorted from biggest to smallest, take the name of \textit{singluar values} and represent the variance of the coordinates defined by the basis vectors in $\bb{R}$ over the $N_M$ models. The idea behind PCA is to project the feature space onto the first $N_c<N_d$ basis vectors, defined as the rows of $\bb{R}$, which have the largest $\Lambda_i$. The remaining coordinates are discarded, as they are associated with numerical noise with negligible information about cosmology. Because $N_d$ components of the feature vector can each have a different scale (think about $P_{\kappa\kappa}$ for example), in order not to exclude some of them because of their measure units, a whitening operation is usually performed before the SVD. The whitened feature matrix $\bb{D}^W$ is defined to be

\begin{equation}
\label{eq:5:whitening}
D^W_{md} = \frac{D_{md}-\mu_d}{\sigma_d}
\end{equation}
%
In equation (\ref{eq:5:whitening}), we introduced $N_d$ arbitrary location and scale parameters $\mu_d,\sigma_d$, which re--center and normalize $\bb{D}$ so that each dimension has the same magnitude. Popular choices for $\mu_d,\sigma_d$ are, respectively, the mean and standard deviation of the rows of $\bb{D}$ \citep{astroMLText}. After the whitening operation, we perform the SVD of $\bb{D}^W$ and calculate the singular values $\bb{\Lambda}$ and the basis vectors $\bb{R}$. We then select the $N_c$ biggest singular values and define $\bb{R}(N_c)$ as the matrix made by the first $N_c$ rows of $\bb{R}$. We project the image feature $\bb{d}$ from the high dimensional space to the lower dimensional space using PCA via a matrix multiplication: 

\begin{equation}
\label{eq:5:pca-projection}
\bb{d}_{\rm PCA}(N_c) = \bb{R}(N_c)\left(\frac{\bb{d}-\pmb{\mu}}{\pmb{\sigma}}\right)
\end{equation}
%
Because the PCA projection changes with different specifications of the external parameters $\mu_d,\sigma_d$, this type of technique is not scale invariant. Nevertheless, we will see it will prove useful in parameter inferences from WL. Modern surveys, such as LSST \citep{LSST}, are planning to use redshift tomography of image features in order to get tight constraints on cosmology. Tomography greatly increases the dimensionality of the feature space. If tracer galaxies are divided in $N_z$ redshift bins, the feature dimensionality $N_d$ increases at least by a factor of $N_z$, which can become $N_z^2$ if the case we consider cross power spectra of $\kappa$ across different $z$ bins. Figure \ref{fig:5:pca} shows an application of PCA to forecasts of $\Lambda$CDM parameter error bars obtained from redshift tomography of an LSST--like survey \citep{PetriPhotoZ}. The Figure shows that using the projection of single redshift features, even if the dimensionality is halved, we still get a constraint on $w_0$ which is within $\sim 15-20\%$ of the value we get in the non--projected case. We achieve even better performance in the tomography case. A wide variety of dimensionality reduction techniques have been proposed in the literature (see \citep{astroMLText} for a non--comprehensive list), and their application to WL cosmology will be investigated in future work.  

\section{Constraints from Weak Lensing}
\label{sec:5:constraints}
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{Figures/png/wl_constraints_single_Om0-sigma8.png} \includegraphics[scale=0.35]{Figures/png/wl_constraints_single_Om0-w0.png}
\includegraphics[scale=0.35]{Figures/png/wl_constraints_combine_Om0-sigma8.png} \includegraphics[scale=0.35]{Figures/png/wl_constraints_combine_Om0-w0.png}
\end{center}
\caption{$1\sigma$ (68\% confidence level) constraints on $(\Omega_m,\sigma_8)$ (left) and $(\Omega_m,w_0)$ (right) from a single $(3.5\,{\rm deg})^2$ WL field of view. The elliptical contours are based on the parameter covariance matrix (\ref{eq:5:linapprox-cov}). The feature covariance matrix $\bbh{C}$ has been estimated from 1000 realizations of the fiducial cosmology and has been corrected for the bias in (\ref{eq:5:pcov-est-1-bias}). In the top panels we show the constraints obtained using the following features: $P_{\kappa\kappa}$ ($\ell \in [10^2,10^5],N_d=100$), the 9 $\kappa$ moments described in \S~\ref{sec:4:moments}, Minkowski functionals ($\kappa_0\in[-2\sigma,2\sigma], N_d=100$) and peak counts ($\kappa_0\in[-2\sigma,5\sigma], N_d=100$). In the bottom panels we show constraints obtained by combining $P_{\kappa\kappa}$ with each of the other three features. Shape noise for a constant source redshift $z_s=2$ with $n_g=15\,{\rm galaxies/arcmin}^2$ has been included.}
\label{fig:5:wlconstraints}
\end{figure}
%
In this section we want to give the reader an idea of the constraining power of WL on $\Lambda$CDM. Moreover, we want to show higher order image features (described in Chapter \ref{chp:4}) complement the $\kappa$ power spectrum adding new information about cosmology. Error bar forecasts on $(\Omega_m,\sigma_8)$ and $(\Omega_m,w_0)$ are shown in Figure \ref{fig:5:wlconstraints}. The Figure displays constraints from both individual and combined features where the combinations include higher order statistics and the power spectrum. The bias on the $\bb{\Sigma}$ estimate that arises from high $N_d$ (see equation (\ref{eq:5:pcov-est-1-bias})) has been corrected for. We can see that, for source galaxies place at constant redshift with an angular density of $n_g=15\,{\rm galaxies/arcmin^2}$, $\kappa$ peak counts and moments have a constraining power which is comparable with $P_{\kappa\kappa}$. We also observe that Minkowski functionals deliver constraints which are about a factor of 2 better than the ones provided by the moments alone (a hint about the fact that these two descriptors are not equivalent can be found in Chapter \ref{chp:4}).

Regarding the moments of $\kappa$, we can see that most of the cosmological information is contained in moments with $\kappa$ gradients (i.e. $\mu_{m}^{(n)}$ with $m>0$) which probe small scale spatial correlations in addition to the PDF of $\kappa$.

From the bottom panel of Figure \ref{fig:5:wlconstraints}, we observe that higher order features complement $P_{\kappa\kappa}$ in giving better constraints on cosmology. The error bar improvement can be as big as a factor of 2 for the Minkowski functionals combined with the power spectrum. In the next Chapter we study a real WL dataset and its cosmological information.      

%\bibliography{ref}