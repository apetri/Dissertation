%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Cosmological parameter inference}
\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\rightmark}}
 \thispagestyle{plain}
\setlength{\parindent}{10mm}
\label{chp:5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this Chapter we give an overview and discuss the cosmological parameter inference techniques used in this work. Inferring $\Lambda$CDM parameters from observations requires the construction of $\kappa$ images from $\pmb{\gamma}$ catalogs and the measurement of a feature $\bb{d}$ from the reconstructed image. When a forward model $\bb{d}(\bb{p})$ that connects features to cosmological parameters $\bb{p}$ is available, an estimate of the parameters $\bbh{p}$ can be derived from an estimate of the feature $\bbh{d}$ in a Bayesian framework. In this Chapter we review the probabilistic framework and we study WL parameter constraining capabilities using the features discussed in Chapter \ref{chp:4}. We also discuss a variety of constraint degradation sources and propose possible remedies.    

\section{Bayesian formalism}
\label{sec:5:bayes}
In this paragraph we review the Bayesian probabilistic framework that we base the parameter inference on. We denote with $\bb{d}$ an $N_d$--dimensional image feature and with $\bb{p}$ a $N_\pi$ dimensional tuple of $\Lambda$CDM cosmological parameters (see Table \ref{tab:1:cosmopar}). We also indicate as $\bbh{d}$ an estimate of a feature from a simulated $\kappa$ field of view, as $\dobs$ a measured feature from an actual observation and as $\bbh{p}$ the deriving parameter estimate. We assume the existence of a forward model $\bb{d}(\bb{p})$ which can be obtained using our WL simulation pipeline or, in special cases such as for the $\kappa$ power spectrum, using analytical codes such as NICAEA \citep{Nicaea}. Using Bayes theorem, the likelihood $\lik{\bbh{p}}{\dobs}$ of an estimate $\bbh{p}$ given an observation $\dobs$ is given by

\begin{equation}
\label{eq:5:bayesthm}
\lik{\bbh{p}}{\dobs,\bb{d}(\bb{p})} = \frac{\lik{\dobs}{\bbh{p},\bb{d}(\bb{p})}\Pi(\bbh{p})}{\mathcal{L}(\dobs)}
\end{equation}
%
In the notation of equation (\ref{eq:5:bayesthm}) $\Pi$ encodes prior information on the parameters coming from WL independent probes (such as CMB experiments for example) and $\mathcal{L}(\dobs)$ is the overall observation likelihood, which acts as a $\bb{p}$--independent normalization factor in the parameter likelihood, which we will ignore in the prosecution of this work. We make a Gaussian assumption for the feature likelihood 

\begin{equation}
\label{eq:5:gaussfeatlik}
\lik{\dobs}{\bbh{p},\bb{d}(\bb{p})} = \frac{1}{(2\pi)^{N_d/2}\vert\bb{C}\vert^{1/2}}\exp\left(-\frac{1}{2}(\dobs-\bb{d}(\bb{p}))^T\bb{C}^{-1}(\dobs-\bb{d}(\bb{p}))\right) 
\end{equation}
%
where $\bb{C}$ is a $\bb{p}$--independent feature--feature covariance matrix. The Gaussian assumption for the data likelihood is justified by the Central Limit Theorem applied to feature averaging over a large number of fields of view, which can be as high as $O(10^3)$ for a $\theta_{\rm FOV}=3.5\,{\rm deg}$ sized tiling of a modern galaxy survey such as LSST. The independence of the covariance matrix $\bb{C}$ on the cosmological parameters is not justified in the present work. The drawbacks of such assumption will be reserved for future investigation. 

Once the parameter likelihood is known, parameter confidence intervals can be obtained looking at surfaces with constant $\mathcal{L}$ in $\bb{p}$ space. We define an $N\sigma$ parameter confidence interval as the $\bb{p}$ space region with $\mathcal{L}>\mathcal{L}_N$. The likelihood confidence levels are defined as 

\begin{equation}
\label{eq:5:liklevel}
\int_{\mathcal{L}>\mathcal{L}_N}\lik{\bbh{p}}{\dobs,\bb{d}(\bb{p})} d\bbh{p} = \frac{1}{\sqrt{2\pi}}\int_{-N}^Ne^{-x^2/2}dx
\end{equation}  
%
Note that this definition of $N\sigma$ confidence intervals corresponds to the commonly accepted one when $\mathcal{L}(\bbh{p})$ is Gaussian. If this is the case, calling $\bbh{p}_0$ the location of the likelihood peak, the matrix $\bb{\Sigma}$, defined by

\begin{equation}
\label{eq:5:parcov}
\left(\Sigma^{-1}\right)_{\alpha\beta} = -\left(\frac{\partial^2\log\mathcal{L}(\bbh{p})}{\partial\h{p}_\alpha\partial\h{p}_\beta}\right)_{\bbh{p}=\bbh{p}_0}
\end{equation}
%
is the parameter estimate covariance matrix. We observe that, even if the parameter likelihood is not Gaussian, we can use the peak location $\bbh{p}_0$ and the matrix (\ref{eq:5:parcov}) as an estimate of the parameters and their covariance, although a complete characterization of the parameter space through the confidence intervals defined in (\ref{eq:5:liklevel}) is preferred. The confidence intervals can be calculated by drawing samples from $\mathcal{L}(\bbh{p})$ using Markov Chain Monte Carlo (MCMC) techniques, which are implemented by many convenient software packages (see for example \citep{emcee}). 

\subsection{Fisher matrix approximation}
Parameter inference becomes simpler if the forward model $\bb{d}(\bb{p})$ is linear in the parameters. Linearity can be safely assumed in the limit in which confidence intervals are expected to be localized around the peak of the likelihood, which is the case for large scale WL surveys. We can write 

\begin{equation}
\label{eq:5:linapprox}
\bb{d}(\bb{p}) = \bb{d}_0 + \bb{M}(\bb{p}-\bb{p}_0) + O(\vert\bb{p}-\bb{p}_0\vert^2)
\end{equation}          
%
Assuming a flat prior $\Pi(\bbh{p})$ and plugging (\ref{eq:5:linapprox}) into (\ref{eq:5:gaussfeatlik}) we get, for the $\bb{p}$--dependent part of the likelihood

\begin{equation}
\label{eq:5:linapprox-lik}
-2\log\mathcal{L}(\bb{p}) = \left[\dobs-\bb{d}_0-\bb{M}(\bb{p}-\bb{p}_0)\right]^T\bb{\Psi}\left[\dobs-\bb{d}_0-\bb{M}(\bb{p}-\bb{p}_0)\right]
\end{equation}
%
From (\ref{eq:5:linapprox-lik}) we can immediately get an estimate for the peak of the likelihood $\bbh{p}_0$ and for the parameter covariance $\bb{\Sigma}$ using (\ref{eq:5:parcov})

\begin{equation}
\label{eq:5:linapprox-peak}
\bbh{p}_0 = \bb{p}_0 + \left(\bb{M}^T\bb{\Psi}\bb{M}\right)^{-1}\bb{M}^T\bb{\Psi}\left(\dobs-\bb{d}_0\right)
\end{equation}
%
\begin{equation}
\label{eq:5:linapprox-cov}
\bb{\Sigma} = \bb{F}^{-1} \equiv \left(\bb{M}^T\bb{\Psi}\bb{M}\right)^{-1} 
\end{equation}
%
Equations (\ref{eq:5:linapprox-peak}), (\ref{eq:5:linapprox-cov}) constitute an important result which takes the name of Fisher matrix approximation, and $\bb{F}\equiv\bb{M}^T\bb{\Psi}\bb{M}$ takes the name of Fisher information matrix. In the case where prior information on the parameters is available, the estimates for the likelihood peak and parameter covariance are modified. For a Gaussian prior with distribution

\begin{equation}
\label{eq:5:gaussprior}
\Pi(\bb{p}) = \frac{\vert\bb{F}_\Pi\vert^{1/2}}{(2\pi)^{N_\pi/2}}\exp\left(-\frac{1}{2}(\bb{p}-\bb{p}_\Pi)^T\bb{F}_\Pi(\bb{p}-\bb{p}_\Pi)\right)
\end{equation}
%
we have 

\begin{equation}
\label{eq:5:linapprox-peak-prior}
\bbh{p}_0 = \left(\bb{F}+\bb{F}_\Pi\right)^{-1}\left[\bb{F}_\Pi\bb{p}_\Pi + \bb{F}\bb{p}_0 + \bb{M}^T\bb{\Psi}(\bbh{d}-\bb{d}_0) \right]
\end{equation}
%
\begin{equation}
\label{eq:5:linapprox-cov-prior}
\bb{\Sigma} = \left(\bb{F}+\bb{F}_\Pi\right)^{-1} 
\end{equation}
%
Equation (\ref{eq:5:linapprox-cov-prior}) simply states that, because the parameter prior is independent from the WL observation, parameter error bars add as their inverse squared weight. In the case in which the parameter likelihood and prior peak at the same location $\bb{p}_0=\bb{p}_\Pi$, equation (\ref{eq:5:linapprox-peak-prior}) reduces to (\ref{eq:5:linapprox-peak}) with a modified Fisher information $\bb{F}+\bb{F}_\Pi$.    

\section{Error degradation induced by noisy covariance}
\label{sec:5:degrade}
In the previous derivation of parameter estimates (\ref{eq:5:linapprox-peak}) and error bars based on the covariance (\ref{eq:5:linapprox-cov}) we have assumed perfect knowledge of the feature--feature covariance matrix $\bb{C}$ and its inverse $\bb{\Psi}$. Although smooth models exist for covariances of power spectra (see (\ref{eq:4:powercov-gauss}) for an example in the Gaussian case), the same is not true for more complicated features, such as the ones described in Chapter \ref{chp:4}. What we are forced to do, in practice, is to obtain an estimate $\bbh{C}$ of $\bb{C}$ from our simulations and to plug the estimate this estimate feature likelihood (\ref{eq:5:gaussfeatlik}). The uncertainties in $\bbh{C},\bbh{\Psi}$ carry over all the way to the parameter estimate $\bbh{p}_0$ and covariance $\bb{\Sigma}$, of which we are left with a noisy estimate $\bbh{\Sigma}$. If simulations and observations are uncorrelated, the parameter estimate $\bbh{p}_0$ is unbiased (within the limits of the linear approximation (\ref{eq:5:linapprox})). The parameter covariance estimator

\begin{equation}
\label{eq:5:pcov-est-1}
\bbh{\Sigma}_1 = \bbh{F}^{-1}
\end{equation}
%
is a biased estimate of $\bb{\Sigma}$ as we will see later in the Chapter. The unbiased version of (\ref{eq:5:pcov-est-1}) is the correct error--bar to assign to $\bbh{p}_0$ if the scatter of the estimator (\ref{eq:5:linapprox-peak}) corresponds to $\Sigma$. Unfortunately we are going to see that this is not true. The real scatter of (\ref{eq:5:linapprox-peak}), assuming $\langle\dobs-\bb{d}_0\rangle=0$ for simplicity, is given by

\begin{equation}
\label{eq:5:peak-scatter}
\left\langle\delta\bbh{p}_0\delta\bbh{p}_0^T\right\rangle = \left\langle\bbh{F}^{-1}\bb{M}^T\bbh{\Psi}(\dobs-\bb{d}_0)(\dobs-\bb{d}_0)^T\bbh{\Psi}\bb{M}\bbh{F}^{-1}\right\rangle
\end{equation}
%
where the expectation value has to be taken both with respect to the observations and the simulations, both affected by noise, but assumed uncorrelated. To have an idea of the magnitude of (\ref{eq:5:peak-scatter}), we can take the expectation value over the observation and focus ourselves on the uncertainty introduced by the noise in the simulations only. We will use 

\begin{equation}
\label{eq:5:expobs}
\left\langle(\dobs-\bb{d}_0)(\dobs-\bb{d}_0)^T\right\rangle = \bb{C}
\end{equation}
%
to produce a noisy estimate of the $\bbh{p}_0$ scatter $\bbh{\Sigma}_2$, defined as 

\begin{equation}
\label{eq:5:pcov-est-2}
\bbh{\Sigma}_2 = \bbh{F}^{-1}\bb{M}^T\bbh{\Psi}\bb{C}\bbh{\Psi}\bb{M}\bbh{F}^{-1} 
\end{equation} 
%
In the next section we are going to show how expectation values of (\ref{eq:5:pcov-est-1}), (\ref{eq:5:pcov-est-2}) over simulations can be calculated under a Gaussian assumption.    

\subsection{Covariance matrix estimation}
To produce estimates of the feature--feature covariance matrix $\bb{C}$, we use our simulation pipeline, described in Chapter \ref{chp:3}, to produce multiple pseudo--independent realizations of a $\kappa$ field of view. We measure the feature $\bbh{d}_r$ from each image using the techniques exposed in Chapter \ref{chp:4} and we produce a covariance estimator $\bbh{C}$ based on ensembles made of a high number $N_r$ of image realizations

\begin{equation}
\label{eq:5:meanest-sim}
\bbh{d}_{\rm mean} = \frac{1}{N_r}\sum_{r=1}^{N_r}\bbh{d}_r 
\end{equation}
%
\begin{equation}
\label{eq:5:covestest-sim}
\bbh{C} = \frac{1}{n}\sum_{r=1}^{N_r}\left(\bbh{d}_r-\bbh{d}_{\rm mean}\right)^T\left(\bbh{d}_r-\bbh{d}_{\rm mean}\right) 
\end{equation}
%
We indicated as $n=N_r-1$ the effective number of degrees of freedom in the ensemble, which is smaller than $N_r$ because the mean feature $\bbh{d}_{\rm mean}$ is not known and has to be estimated from the ensemble itself. If the feature estimate $\bbh{d}_r$ is drawn from a Gaussian distribution with covariance $\bb{C}$, the covariance estimate $\bbh{C}$ is distributed according to the Wishart probability distribution \citep{Taylor12,Taylor14,MasumotoWishart}. A functional form for the Wishart distribution $\lik{\bbh{C}}{\bb{C},n}$ can be obtained from its characteristic function 

\begin{equation}
\label{eq:5:chardef}
\phi(\bb{J}) = \left\langle e^{i\Tr(\bb{J}\bbh{C})}\right\rangle
\end{equation}
%
Performing an inverse Fourier transform in matrix space (much like the one in (\ref{eq:4:characteristic-inverse})), we can reconstruct $\lik{\bbh{C}}{\bb{C},n}$ from $\phi(\bb{J})$. The characteristic function $\phi$ can be evaluated from the moments of the Wishart distribution which are easily expressed in terms of $\bb{C}$ and $n$ via a straightforward though tedious procedure based on Wick's theorem. The final result is (see \citep{Taylor12} for the details)

\begin{equation}
\label{eq:5:charw}
\phi(\bb{J}) = \left\vert\mathds{1}_{N_d\times N_d}-\frac{2i\bb{JC}}{n}\right\vert^{-n/2}
\end{equation}
%
The inverse Fourier transform leads to the functional form of the Wishart distribution $\lik{\bbh{C}}{\bb{C},n} = \mathcal{W}(\bb{C},\bbh{C},n)$ with

\begin{equation}
\label{eq:5:wishart}
\mathcal{W}(\bb{C},\bbh{C},n) =  \left(\frac{n^{nN_d/2}\vert \bb{C}\vert^{-n/2}\vert\bbh{C}\vert^{(n-N_d-1)/2}}{2^{nN_d/2}\Gamma_{N_d}(n/2)}\right)\exp\left(-\frac{n}{2}\Tr(\bbh{C}\bb{C}^{-1})\right)
\end{equation}
%
where the generalized gamma function $\Gamma_N$ is defined as an integral over the positive semidefinite $N\times N$ symmetric matrices 

\begin{equation}
\label{eq:5:gengamma}
\Gamma_N(x) = \int_{\bb{X}>0}d\bb{X}\vert\bb{X}\vert^{x-(N+1)/2}e^{-\Tr\bb{X}}
\end{equation}
%
We can use the functional form of the Wishart distribution (\ref{eq:5:wishart}) to derive a closed formula for the distribution of the inverse estimate $\bbh{\Psi}=\bbh{C}^{-1}$, which can be done with a change of variables in matrix space. Following \citep{Taylor12} we get $\lik{\bbh{\Psi}}{\bb{\Psi},n} = \mathcal{W}^{-1}(\bbh{\Psi},\bb{\Psi},n)$ with 

\begin{equation}
\label{eq:5:wishart-inv}
\mathcal{W}^{-1}(\bbh{\Psi},\bb{\Psi},n) = \left(\frac{n^{nN_d/2}\vert \bb{\Psi}\vert^{n/2}\vert\bbh{\Psi}\vert^{-(n+N_d+1)/2}}{2^{nN_d/2}\Gamma_{N_d}(n/2)}\right)\exp\left(-\frac{n}{2}\Tr(\bb{\Psi}\bbh{\Psi}^{-1})\right)
\end{equation}
%
Using (\ref{eq:5:wishart-inv}), it can be shown (see \citep{MasumotoWishart}) that the estimate of the inverse covariance $\bbh{\Psi}$ is biased because

\begin{equation}
\label{eq:5:inv-bias}
\left\langle\bbh{\Psi}\right\rangle = \frac{n\bb{\Psi}}{n-N_d-1}
\end{equation} 
%
Because the moments of any square sub--matrix of $\bbh{\Psi}$ can be expressed in terms of the relevant elements of $\bb{\Psi}$, $n$ and the combination $\gamma=n-N_d-1$ (see again \citep{MasumotoWishart}) it can be shown that the rescaled Fisher information estimate $\bbh{F}'$ defined as 

\begin{equation}
\label{eq:5:rescaledF}
\bbh{F}' = \frac{(n+N_\pi-N_d)\bbh{F}}{n}
\end{equation}
%
is distributed as $\lik{\bbh{F}'}{\bb{F}',n,N_\pi,N_d} = \mathcal{W}^{-1}(\bbh{F}',\bb{F}',n+N_\pi-N_d)$. This observation leads immediately to the conclusion that the parameter error estimate based on (\ref{eq:5:pcov-est-1}) is biased low

\begin{equation}
\label{eq:5:pcov-est-1-bias}
\left\langle\bbh{\Sigma}_1\right\rangle = \left(1+\frac{N_\pi-N_d}{n}\right)\bb{\Sigma}
\end{equation}
%
This bias can easily be accounted for by applying the correction factor suggested by equation (\ref{eq:5:pcov-est-1-bias}) to the parameter covariance estimator (\ref{eq:5:pcov-est-1}). This procedure allows to estimate the parameter error bars in an unbiased fashion purely relying on simulations, and hence can be used for obtaining approximate parameter contour forecasts. When analyzing a real observation, though, we are left with a parameter estimate $\bbh{p}_0$, obtained from the peak of the likelihood which, as we will see, is drawn from a distribution with a width larger than $\bb{\Sigma}$. This larger scatter originates from the fact that the feature--feature covariance estimate $\bbh{C}$ is noisy and, although $\bbh{C}$ is unbiased, the peak scatter estimator $\bbh{\Sigma}_2$ in (\ref{eq:5:pcov-est-2}) is not. Unfortunately, unlike the case for $\bbh{\Sigma}_1$, the expectation value of $\bbh{\Sigma}_2$ cannot be calculated exactly and needs to be approximated. We will tackle this issue in the following paragraph.  

\subsection{Perturbative calculation of the estimate scatter}
The expectation value of \ref{eq:5:pcov-est-2} is not easily calculable analytically because both $\bbh{\Psi},\bbh{F}^{-1}$ appear in the expression. In order to evaluate the behavior of the scatter $\bbh{\Sigma}_2$ with $N_d,N_r$, we proceed with a perturbative approach in the quantity $\delta\bbh{\Psi}$, defined by

\begin{equation}
\label{eq:5:deltapsi}
\bbh{\Psi} = \bb{\Psi} + \delta\bbh{\Psi}
\end{equation}
%
With this definition, the expression (\ref{eq:5:pcov-est-2}) can be expanded in a power series in $\delta\bbh{\Psi}$. If the moments of the inverse Wishart distribution are known, we can use them to calculate $\langle\bbh{\Sigma}_2\rangle$ at arbitrary perturbative orders. Using the definition

\begin{equation}
\label{eq:5:deltaF}
\delta\bbh{F} = \bb{M}^T\delta\bbh{\Psi}\bb{M}
\end{equation}
%
we obtain

\begin{equation}
\label{eq:5:pcov-est-2-pert}
\bbh{\Sigma}_2 = (\bb{F}+\delta\bbh{F})^{-1}\bb{M}^T(\bb{\Psi}+\delta\bbh{\Psi})\bb{C}(\bb{\Psi}+\delta\bbh{\Psi})\bb{M}(\bb{F}+\delta\bbh{F})^{-1}
\end{equation}
%
The series expansion for $(\bb{F}+\delta\bbh{F})^{-1}$, given by

\begin{equation}
\label{eq:5:seriesdF}
(\bb{F}+\delta\bbh{F})^{-1} = \sum_{k=0}^\infty (-1)^k(\bb{F}^{-1}\delta\bbh{F})^k\bb{F}^{-1}
\end{equation}
%
provides a straightforward, although computationally tedious way to express (\ref{eq:5:pcov-est-2-pert}) at the desired order in $\delta\bbh{\Psi}$. \citep{MasumotoWishart} showed that a perturbation expansion in $\delta\bbh{\Psi}$ is roughly equivalent to a pertubration series in $1/N^r$, where higher connected moments of the inverse Wishart distribution give rise to higher powers in $1/N_r$. In order to express the expectation value of (\ref{eq:5:pcov-est-2-pert}) at $O(1/N_r^2)$, we need to know the moments of $\mathcal{W}^{-1}$ up to quartic order \citep{PetriVariance}. Following \citep{MasumotoWishart}, we have

\begin{equation}
\label{eq:5:inv-wishart-moments-2}
\left\langle\delta\h{\Psi}_I\delta\h{\Psi}_J\right\rangle = \frac{n^2\Psi_I\Psi_J + n^2\gamma\Psi_{\{ I}\Psi_{J\}}}{4\gamma^2(\gamma-1)(2\gamma+1)}
\end{equation}
%
\begin{equation}
\label{eq:5:inv-wishart-moments-3}
\left\langle\delta\h{\Psi}_I\delta\h{\Psi}_J\delta\h{\Psi}_K\right\rangle = \frac{n^3\Psi_{\{ I}\Psi_J\Psi_{K\}}}{8\gamma(\gamma-1)(\gamma-2)(\gamma+1)(2\gamma+1)}
\end{equation}
%
\begin{equation}
\label{eq:5:inv-wishart-moments-4}
\left\langle\delta\h{\Psi}_I\delta\h{\Psi}_J\delta\h{\Psi}_K\delta\h{\Psi}_L\right\rangle = \frac{n^4(2\gamma^2-5\gamma+9)\Psi_{\{ I}\Psi_{J\}}\Psi_{\{ K}\Psi_{L\}}}{16\gamma(\gamma-1)(\gamma-2)(\gamma-3)(2\gamma-1)(\gamma+1)(2\gamma+1)(2\gamma+3)}
\end{equation}
%
We adopted the following notation: we indicated with a capital letter $I=(i_1,i_2)$ a pair of indices $i_1,i_2$. We also used the curly brace notation for a symmetrization of indices

\begin{equation}
\label{eq:5:symmind}
\Psi_{\{ I}\Psi_{J\}} = \Psi_{i_1j_1}\Psi_{i_2j_2} + \Psi_{i_1j_2}\Psi_{i_2j_1}
\end{equation}  
%
In equations (\ref{eq:5:inv-wishart-moments-2}), (\ref{eq:5:inv-wishart-moments-3}), (\ref{eq:5:inv-wishart-moments-4}) we kept only the terms which are of order up to $O(1/N_r^2)$. The structure of the expression (\ref{eq:5:pcov-est-2-pert}), coupled to the one of the inverse Wishart moments causes the expectation value $\langle\bbh{\Sigma}_2\rangle$ to be the sum of a series of terms of the form $f_a(N_d,N_\pi)\bb{\Sigma}/N_r^a$, with $f_a$ a polynomial in $N_d,N_\pi$. Each of these polynomials must contain at least one factor proportional to $N_d-N_\pi$ since, for $N_d=N_\pi$, $\langle\bbh{\Sigma}_2\rangle$ must be identically $\bb{\Sigma}$. Expanding (\ref{eq:5:pcov-est-2-pert}) at quadratic, cubic and quartic order in $\delta\bbh{\Psi}$, and after carrying out the tedious calculations, one gets the contributions

\begin{equation}
\label{eq:5:contr-2}
(\delta\bbh{\Psi})^2 \rightarrow \frac{\gamma(N_d-N_\pi)\bb{\Sigma}}{(\gamma-1)(2\gamma+1)}
\end{equation} 
%
\begin{equation}
\label{eq:5:contr-3}
(\delta\bbh{\Psi})^3 \rightarrow -\frac{4(N_d-N_\pi)(1+N_\pi)\bb{\Sigma}}{N_r^2}
\end{equation} 
%
\begin{equation}
\label{eq:5:contr-4}
(\delta\bbh{\Psi})^4 \rightarrow \frac{3(N_d-N_\pi)(1+N_\pi)\bb{\Sigma}}{N_r^2}
\end{equation} 
%
Combining (\ref{eq:5:contr-2}), (\ref{eq:5:contr-3}) and (\ref{eq:5:contr-4}) we finally get 

\begin{equation}
\label{eq:5:pcov-est-2-exp}
\left\langle\bbh{\Sigma}_2\right\rangle = \left(1+\frac{N_d-N_\pi}{N_r}+\frac{(N_d-N_\pi)(N_d-N_\pi+2)}{N_r^2}\right)\bb{\Sigma} + O\left(\frac{1}{N_r^3}\right)
\end{equation} 
%
The result (\ref{eq:5:pcov-est-2-exp}) has an important consequence: although we are able to forecast parameter error bars from simulations in an unbiased way via (\ref{eq:5:pcov-est-1-bias}), when analyzing a real observation the scatter of the likelihood peak $\bbh{p}_0$ will be larger than $\bb{\Sigma}$ by a factor of $\sim 1+N_d/N_r$, when the feature covariance matrix is estimated from an ensemble of $N_r$ realizations. This means that, for high dimensional image features, noise in the covariance matrix will severely degrade the parameter estimate, and the unbiased errorbar forecasted from simulations will be an under--estimate of the real errorbar. \citep{Taylor14} found an empirical formula for $\langle\bbh{\Sigma}_2\rangle$ which is able to accurately reproduce numerical investigations of the parameter degradation

\begin{equation}
\label{eq:5:pcov-ext-2-emp}
\left\langle\bbh{\Sigma}_2\right\rangle_{\rm empirical} = \left(\frac{N_r-2}{N_r+N_\pi-N_d-2}\right)\bb{\Sigma}
\end{equation}
%
Note that (\ref{eq:5:pcov-ext-2-emp}) reduces to (\ref{eq:5:pcov-est-2-exp}) when expanded to order $O(1/N_r^2)$. 
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{Figures/eps/curving_nb.eps}
\end{center}
\caption{Expectation value of the $w$ estimate scatter $\h{\Sigma}_{w_0w_0}$ as a function of the number of pseudo--independent realizations $N_r$ used to measure the feture covariance matrix. We show a variety of features with different dimensionalities, which include $\kappa$ power spectra and peak counts. We show the numerically bootstrapped results from our ensembles as points, the $O(1/N_r),O(1/N_r^2)$ perturbative predictions from equation (\ref{eq:5:pcov-est-2-exp}) as dashed and thin solid lines respectively and the empirical result from equation (\ref{eq:5:pcov-ext-2-emp}) as thick solid lines. The asymptotic parameter covariance $\bb{\Sigma}_\infty$, which coincides with the true covariance $\bb{\Sigma}$, has been estimated with a linear regression of $\langle\bbh{\Sigma}\rangle_2$ versus $1/N_r$ using the large $N_r$ tail.} 
\label{fig:5:curvenb}
\end{figure}
%
Figure \ref{fig:5:curvenb} shows a numerical experiment we performed using ensemble bootstrapping in order to measure the constraint degradation of the Dark Energy equation of state parameter $w_0$. The Figure show that, for ratios $N_d/N_r$ close to unity, the scatter of the $w_0$ estimate can be significantly bigger than the forecasted parameter covariance $\bb{\Sigma}$ (see \citep{DodelsonSchneider13,PetriVariance}). This numerical degradation brings up the necessity of dimensionality reduction: high dimensional features might contain more information about cosmology but, since in general their covariance matrix has to measured from simulations, this will induce constraint degradations that increase the size of the errorbars. A good compromise is to find low dimensional features that retain as much of the cosmological information as possible. This is not an easy task, and some possibilities will be illustrated later in the Chapter.  

\section{Pseudo--independence of realizations}
The analytical results illustrated in \S~\ref{sec:5:bayes}, \S~\ref{sec:5:degrade} depend on the assumption that the image realizations that make up the $\kappa$ ensembles are independent. Because these ensembles are built with the sampling procedure described in \S~\ref{sec:3:sampling}, which uses $N_s$ independent $N$--body simulations to construct a large number $N_r\gg N_s$ of image realizations, these realizations are not guaranteed to be all independent. If $N_s$ is small, on the other hand, it is not guaranteed that $N_s$ simulations with a box size of $L_b=240\,{\rm Mpc}/h$ (such as the ones used in this work) are able to sample cosmic variance in an unbiased way, given the size $\sim 3\,{\rm Gpc}$ of the present Hubble horizon. 
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{Figures/eps/ps_pdf.eps}
\end{center}
\caption{Distribution of the $\kappa$ power spectrum at four distinct multipoles. We show the distribution of $P_{\kappa\kappa}$ over 1000 realization ensembles constructed with different $N_s$ (solid colored lines). We also show the distribution of $P_{\kappa\kappa}$ over an ensemble built from a single $N$--body box, from which a large number of realizations $N_r\sim 10^5$ has been drawn (dashed black line).}
\label{fig:5:pspdf}
\end{figure}
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{Figures/eps/means_ns.eps} \includegraphics[scale=0.4]{Figures/eps/scaling_ns.eps}
\end{center}
\caption{Top panel: mean feature measured from $N_r=1000$ realization ensembles build with varying $N_s$. We show the $\kappa$ power spectrum measured at three selected mmultipoles $\ell$ and the peak counts at three different thresholds $\kappa_0$ (colored lines). The mean feature is plotted in units of the statistical error measured from the ensemble variance. We also show as a dashed black line the tolerance level corresponding to 10\% of the statistical error. Bottom panel: asymptotic parameter variance on $w_0$ plotted against the number of independent simulations $N_s$ used to construct the ensembles. The asymptotic variance has been obtained linearly regressing the boostrapped $\langle\bbh{\Sigma}_2\rangle$ against $1/N_r$ for large $N_r$. The trends are shown in units of the mean over $N_s$ for two different binning choices of the power spectrum (black, red) and for the peak counts (green).}
\label{fig:5:mcns}
\end{figure}
%
One of the effects of this biased sampling is evident in Figure \ref{fig:5:pspdf}, which shows the distribution of the $\kappa$ power spectrum in selected multipoles among the different realizations that populate the ensemble. In particular we can see that, when the sampling is based on a single simulation ($N_s=1$), the peak location of the distribution of $P_{\kappa\kappa}(\ell)$ depends on which particular ensemble is selected. Ensembles constructed from different $N$--body boxes sample the small scale power spectrum ($\ell\gtrsim 1000$) differently, while the distinction is not evident on larger scales ($\ell\lesssim 400$). Figure \ref{fig:5:pspdf} also shows that, with our chosen box size, $N_s=5$ is sufficient to obtain an unbiased sampling of the power spectrum up to $\ell\approx5000$. Another representation of biased sampling of image features is shown in the top panel of Figure \ref{fig:5:mcns}, which shows the mean of selected elements of the feature vectors as a function of the number of simulations used in the sampling. The plot shows that for features that are sensitive to $\delta$ fluctuations in the linear regime, such as large scale power spectra and low threshold peak counts, as few as $N_s=2$ simulations (with $L_b=240\,{\rm Mpc}/h$) are sufficient for the sampling bias to be smaller than 10\% of the statistical error. For more non--linear features, such as small scale power spectra and high threshold peak counts the reqyuired number of $N$--body simulations is of the order of 10, referred to the same 10\% tolerance mentioned before.    
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{Figures/eps/scaling_nr.eps}
\end{center}
\caption{$w_0$ constraint degradation as a function of $N_r$ for different feature choices, measured from ensembles buit with $N_s=1$. The asymptotic variance $\bb{\Sigma}_{w_0w_0,\infty}$ has been estimated as the ensemble variance calculated at $N_r=128,000$. We show the trends for small (black) and large (blue) scale power spectra and large threshold peak counts for unsmoothed (green) and $1'$ smoothed (red) $\kappa$ images.}
\label{fig:5:pseudonr}
\end{figure}
%
Figure \ref{fig:5:mcns} also shows, in the lower panel, that the pseudo--independence of $\kappa$ realizations caused by small $N_s$ does not affect the sampling of the asymptotic parameter variance $\bb{\Sigma}$, which is already obtained correctly calculated for $N_s=2$ (modulo statistical fluctuations). This is true even if, for large $N_r$, the image realizations cannot be considered all independent, as Figure \ref{fig:5:pseudonr} shows. The Figure highlights the fact that, when the sampling procedure is based on a single $N$--body simulations, image features measured from the resulting $\kappa$ ensemble start to be correlated for large $N_r$. Depending on which feature we consider, when we look at the error bar  degradation $\langle\bbh{\Sigma}_2\rangle-\bb{\Sigma}$, in the case of perfect independence we should observe a $1/N_r$ asymptotic behavior. What we measure, on the other hand, is a flattening of the degradation trend at high $N_r$, hinting that the realizations that make up the ensemble are correlated. This flattening manifests itself at different $N_r$ depending on which feature we are looking at. Large scale $\kappa$ power spectra drawn from a single box are independent for $N_r<{\rm few}\times 10^3$, while smaller scale statistics like small scale power spectra and peak counts are independent for $N_r<{\rm few}\times 10^4$. These numbers shoule be interpreted as orders of magnitude, and are referred to the size of the boxes $L_b=240\,{\rm Mpc}/h$ used in this work. Different box sizes will likely modify the scale $N_r$ on which deviations from perfect independence start to appear.   


\section{Dimensionality reduction}

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{Figures/eps/w_power_spectrum_pca.eps} \includegraphics[scale=0.35]{Figures/eps/w_peaks_pca.eps}
\includegraphics[scale=0.35]{Figures/eps/w_moments_pca.eps}
\end{center}
\caption{}
\label{fig:5:pca}
\end{figure}

\section{Born approximation induced bias}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{Figures/eps/bornBias_ngal_convergence_momentsSN_s50_nb9.eps}
\end{center}
\caption{}
\label{fig:5:biasng}
\end{figure}

\section{Weak Lensing constraining power}

\bibliography{ref}