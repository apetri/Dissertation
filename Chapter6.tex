%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{An application to real data: the CFHTLenS galaxy survey}
\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\rightmark}}
 \thispagestyle{plain}
\setlength{\parindent}{10mm}

\label{chp:6}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this Chapter we present an application of the techniques presented so far to real data, contained in the publicly available CFHTLenS survey catalogs \citep{cfht1,cfht2,CFHTKilbinger}. We start by reviewing the data reduction procedure that bridges the gap between the row--ordered data to the reconstructed $\kappa$ maps. We then present a set of cosmological simulations tailored to the CFHTLenS catalogs, which allow us to build the feature emulator used to produce $\Lambda$CDM parameter inferences.   

\section{Catalogs}
The CFHTLenS survey covers an area of 154\,${\rm deg}^2$, divided in four sub--patches of size 64,23,44 and 23\,${\rm deg}^2$. The publicly released catalogs, created with the SExtractor software \citep{SExtractor}, contain information on  galaxy photometric redshifts (see \citep{cfhtPhoto} for a detail on the estimation procedure) and shapes extracted with \ttt{lensfit} \citep{cfht1,cfht2}. After applying a redshift cut $0.2<z<1.3$ on the galaxies, and after considering only the ones with positive weight w (larger w indicates smaller shape measurement uncertainty), we are left with roughly $N_g=4.2$\,million objects distributed over an area of 124.7\,${\rm deg}^2$. This corresponds to an average galaxy density of $n_g\approx 9.3\,{\rm galaxies/arcmin}^2$. The catalog size is further reduced by $25\%$ if sub--patches with non negligible star--galaxy correlations are rejected \citep{CFHTFu}. These correlations are caused by imperfect Point Spread Function (PSF) removal procedures. With the information contained in the publicly available catalogs, we can construct the CFHTLenS $\kappa$ maps using the KS procedure (\ref{eq:2:kappa-ks}) applied to the ellipticity estimated shear data. We create smooth ellipticity maps according to \citep{PetriCFHTMink,PetriCFHTPeaks}

\begin{equation}
\label{eq:6:ellip-smooth}
\bar{\bb{e}}(\pt) = \frac{\sum_{i=1}^{N_g}W(\vert\pt-\pt_i\vert){\rm w}_i(\bb{e}_i -\bb{c}_i)}{\sum_{i=1}^{N_g}W(\vert\pt-\pt_i\vert){\rm w}_i(1+m_i)}
\end{equation}
%
In equation (\ref{eq:6:ellip-smooth}), $\pt_i,{\rm w}_i,\bb{e}_i,\bb{c}_i,m_i$ refer, respectively, to the sky position, weight, observed ellipticity, additive and multiplicative ellipticity correction of the $i$--th galaxy as read from the catalog. A Gaussian smoothing window

\begin{equation}
\label{eq:6:gausswin}
W(\pt) = \frac{1}{2\pi\theta_G^2}\exp\left(-\frac{\theta^2}{2\theta_G^2}\right)
\end{equation}
%
has been applied to the reconstructed images. The window size $\theta_G$ has been fixed to 1$'$, and occasionally varied to 1.8$'$ and 3.5$'$ for testing purposes, described later in the Chapter. We then set $\pmb{\gamma}(\pt)=\bar{\bb{e}}(\pt)$ and use equation (\ref{eq:2:kappa-ks}) to construct the $\kappa$ images which will then be used for the parameter inferences. We divide the survey area in 13 square sub--fields of area $12\,{\rm deg}^2$. We sample each subfield with $512^2$ evenly spaced square pixels. The reduced data undergoes a feature extraction step (see Chapter \ref{chp:4}). The extracted features are then compared to the simulated ones in a Bayesian fashion (see Chapter \ref{chp:5}) to obtain $\Lambda$CDM parameter posterior distributions. In the next section we describe the simulations used for the construction of the CFHTLenS feature emulator.      

\section{Emulator}

\subsection{Cosmological parameter sampling}
\label{sec:6:sampling}
We describe the procedure we used to sample the $\Lambda$CDM parameter space. We consider a subset of $N_\pi=3$ parameters $\bb{p}=(\Omega_m,w_0,\sigma_8)$ and we seek a way to uniformly sample it so that no parameter is repeated twice. This sampling scheme takes the name of \textit{latin hypercube} \citep{Coyote2}. One way to implement the latin hypercube sample in practice is to set a $N_\pi$--dimensional bounding box that will contain all the sampled points, and normalize it to $[0,1]^{N_\pi}$ for simplicity. We can then set the number $N_M$ of cosmological models we wish to sample and distribute in an uniform latin hypercube scheme. Following \citep{Coyote2,PetriCFHTMink} we define a cost function 

\begin{equation}
\label{eq:6:cost}
\mathcal{C}(\bb{P}) = \frac{2{N_\pi}^{1/2}}{N_M(N_M-1)}\sum_{i<j} \frac{1}{\vert \bb{P}_i-\bb{P}_j \vert}
\end{equation} 
%
where $\bb{P}$ is a $N_M\times N_\pi$ matrix that contains the information on the sampled points and the sum runs over all pair of points. In order to sample the hypercube uniformly, we seek a configuration $\bb{P}$ that minimizes the cost function (\ref{eq:6:cost}) while enforcing the latin hypercube constraint. Because $\mathcal{C}$ is effectively the Coulomb potential energy of $N_M$ unit point charges confined in a box, its minimum leads to a uniform configuration. The simplest latin hypercube arrangement corresponds to the diagonal design $\bb{P}^0$, in which the points are arranged on the diagonal of the hypercube

\begin{equation}
\label{eq:6:diagonal}
\bb{P}^0_{i} = \frac{i}{N_M}\underbrace{(1,1,...,1)}_{N_\pi}
\end{equation}
%  
Of course this trivial arrangement is far from optimal. A possible heuristic method to find out a configuration $\bb{P}$ that minimizes (\ref{eq:6:cost}) is simulated annealing \citep{Skiena}, which however is too computationally expensive for our purposes. We resort on this less accurate, but faster, heuristic scheme instead: 
\begin{enumerate}
\item Start from the diagonal design $\bb{P}^0$
\item Pick a random pair of points $(i,j)$ among the $N_M(N_M-1)/2$ available, pick a random parameter $p$ among the $N_\pi$ available
\item Swap $P_{ip}$ with $P_{jp}$ (the swap preserves the latin hypercube property), recompute the cost function $\mathcal{C}$
\item If the cost decreases, keep the swap, otherwise revert to the previous configuration
\item Re--iterate the procedure starting from point 2. 
\end{enumerate}
%
\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{Figures/eps/cfht_design.eps}
\end{center}
\caption{Distribution of the $(\Omega_m,w_0,\sigma_8)$ sampled triplets in the parameter prior box. We show both the $(\Omega_m,w_0)$ (left) and the $(\Omega_m,\sigma_8)$ (right) projections. The black points correspond to the $N_M=91$ latin hypercube models, and the red cross correspond to the fiducial $\Lambda$CDM parameters shown in Table \ref{tab:1:cosmopar}. The design is the result of $10^5$ iterations of the heuristic procedure described in \S~\ref{sec:6:sampling}.}
\label{fig:6:sampling}
\end{figure}
%
After several iterations, we are left with a latin hypercube design which samples the parameter space approximately uniformly. The last step is to rescale the parameter coordinates from the $[0,1]$ bounds to their originally intended values. The latin hypercube design we use for the present analysis is shown in Figure \ref{fig:6:sampling}. 

\subsection{Simulations}

\subsection{Interpolation}

\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{Figures/eps/cfht_emulator_accuracy.eps}
\end{center}
\caption{}
\label{fig:6:interpolation}
\end{figure}



\section{$\Lambda$CDM parameter inference}

\subsection{PCA projection}

\subsection{Density fluctuations}

\subsection{Dark Energy}


\bibliography{ref}